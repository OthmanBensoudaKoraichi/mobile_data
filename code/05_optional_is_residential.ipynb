{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook is optional. We were testing the creation of a variable which assesses whether a certain cluster is residential or not. It turns out to be too expensive and to add very little value.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter dataset (first 5 clusters for each mobile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cluster_quarterly_metrics = pd.read_csv(\"../datasets/cluster_quarterly_metrics.csv\")\n",
    "\n",
    "\n",
    "filtered_data = (\n",
    "    cluster_quarterly_metrics\n",
    "    .sort_values(['caid', 'quarter', 'total_pings'], ascending=[True, True, False])\n",
    "    .groupby(['caid', 'quarter'])\n",
    "    .head(5)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "filtered_data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get geocoding from geocoding API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each cluster, we have the latitude and longitude. However, we don't have the address name (e.g. 454 Ruthven Avenue). We get this info thanks to Google's geocoding API.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv('GOOGLE_MAPS_API_KEY')\n",
    "\n",
    "# Initialize session for faster requests\n",
    "session = requests.Session()\n",
    "\n",
    "# Function to retrieve address from coordinates\n",
    "def get_address_from_coords(lat, lng, api_key, max_retries=5):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?latlng={lat},{lng}&key={api_key}\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                if result['status'] == 'OK':\n",
    "                    return result['results'][0]['formatted_address']\n",
    "                elif result['status'] == 'OVER_QUERY_LIMIT':\n",
    "                    delay = 5 + random.uniform(1, 3)  # Controlled delay for retry\n",
    "                    print(f\"⚠️ Rate limit hit. Retrying in {delay:.2f} seconds...\")\n",
    "                    time.sleep(delay)\n",
    "                    retries += 1\n",
    "                else:\n",
    "                    return f\"Error: {result['status']}\"\n",
    "            else:\n",
    "                return \"Failed to connect to API\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    return \"Failed after max retries\"\n",
    "\n",
    "# Optimized function with controlled delays\n",
    "def fetch_addresses_with_throttling(data, batch_size=10, delay_between_batches=1):\n",
    "    addresses = []\n",
    "\n",
    "    for i in tqdm(range(0, len(data), batch_size), desc=\"Fetching Addresses\"):\n",
    "        batch = data.iloc[i:i + batch_size]\n",
    "        for index, row in batch.iterrows():\n",
    "            address = get_address_from_coords(row['centroid_latitude'], row['centroid_longitude'], api_key = \"AIzaSyDBASVfTsyh5kjcFsXSQANUEsu2fPgyoDg\")\n",
    "            addresses.append(address)\n",
    "\n",
    "        # Introduce a short delay after each batch\n",
    "        time.sleep(delay_between_batches + random.uniform(0.5, 1.5))  # Slight randomness for safety\n",
    "\n",
    "    return addresses\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add address column using throttled fetching\n",
    "filtered_data['address'] = fetch_addresses_with_throttling(filtered_data, batch_size=10, delay_between_batches=1)\n",
    "\n",
    "# Save filtered data to CSV\n",
    "filtered_data.to_csv('../datasets/filtered_data_with_addresses.csv', index=False)\n",
    "\n",
    "print(\"✅ Descriptions successfully extracted and saved to 'filtered_data_with_addresses.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filtered_data = pd.read_csv('../datasets/filtered_data_with_addresses.csv')\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1 : Google Search API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape descriptions (Google Search API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now that we have the address (e.g. 454 Ruthven Avenue), we can search for it on Google.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import threading\n",
    "\n",
    "# Google Custom Search API credentials\n",
    "api_key = 'AIzaSyBe4OdpVVJsbmsDDL9B89NkPFe1DY8bVmo'\n",
    "cx = '83a75dd1ff5f3496b'\n",
    "\n",
    "# File paths\n",
    "file_path = '../datasets/filtered_data_with_google_results.csv'\n",
    "source_path = '../datasets/filtered_data_with_addresses.csv'\n",
    "\n",
    "# Load dataset\n",
    "if os.path.exists(file_path):\n",
    "    filtered_data = pd.read_csv(file_path)\n",
    "    if 'search_results' not in filtered_data.columns:\n",
    "        filtered_data['search_results'] = \"\"\n",
    "else:\n",
    "    filtered_data = pd.read_csv(source_path)\n",
    "    filtered_data['search_results'] = \"\"\n",
    "\n",
    "# Rate limit tracker\n",
    "lock = threading.Lock()\n",
    "timestamps = []\n",
    "\n",
    "def rate_limited_request():\n",
    "    with lock:\n",
    "        now = time.time()\n",
    "        timestamps.append(now)\n",
    "\n",
    "        # Remove timestamps older than 60 seconds\n",
    "        one_minute_ago = now - 60\n",
    "        while timestamps and timestamps[0] < one_minute_ago:\n",
    "            timestamps.pop(0)\n",
    "\n",
    "        # If we've hit the limit, sleep until allowed\n",
    "        if len(timestamps) >= 100:\n",
    "            wait_time = 60 - (now - timestamps[0]) + random.uniform(0.1, 0.5)\n",
    "            print(f\"⏳ Global rate limit hit. Sleeping for {wait_time:.2f}s...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "# Function to fetch Google search results\n",
    "def fetch_address_info(index, address, max_retries=3):\n",
    "    query = address\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?key={api_key}&cx={cx}&q={query}\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            rate_limited_request()\n",
    "            response = requests.get(url, timeout=10)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                all_results = \"\"\n",
    "                if 'items' in data:\n",
    "                    for item in data['items']:\n",
    "                        title = item.get('title', '')\n",
    "                        link = item.get('link', '')\n",
    "                        snippet = item.get('snippet', '')\n",
    "                        all_results += f\"Title: {title}\\nLink: {link}\\nDescription: {snippet}\\n-----\\n\"\n",
    "                else:\n",
    "                    all_results = \"No results found.\"\n",
    "                return index, all_results\n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                wait_time = 5 + random.uniform(1, 3)\n",
    "                print(f\"⏳ 429 Rate limit from API at index {index}, sleeping {wait_time:.2f}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                return index, f\"Error: {response.status_code}, {response.text}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Exception for index {index}: {e}\")\n",
    "            time.sleep(2 + random.uniform(0.5, 1.5))\n",
    "\n",
    "    return index, \"❌ Failed after retries\"\n",
    "\n",
    "# Filter rows that need processing\n",
    "pending_rows = filtered_data[filtered_data['search_results'].isna() | (filtered_data['search_results'].str.strip() == \"\")]\n",
    "\n",
    "# Multithreaded execution\n",
    "batch_size = 100\n",
    "max_workers = 5\n",
    "results_buffer = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [\n",
    "        executor.submit(fetch_address_info, idx, row['address'])\n",
    "        for idx, row in pending_rows.iterrows()\n",
    "    ]\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Fetching Google Results\"):\n",
    "        idx, result = future.result()\n",
    "        filtered_data.at[idx, 'search_results'] = result\n",
    "        results_buffer.append(idx)\n",
    "\n",
    "        if len(results_buffer) >= batch_size:\n",
    "            filtered_data.to_csv(file_path, index=False)\n",
    "            print(f\"💾 Saved intermediate batch of {batch_size} results.\")\n",
    "            results_buffer = []\n",
    "\n",
    "# Final save\n",
    "filtered_data.to_csv(file_path, index=False)\n",
    "print(f\"✅ Done. Results saved to {file_path}\")\n",
    "\n",
    "print(f\"✅ Done. Results saved to {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.to_excel('../datasets/filtered_data_with_google_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2 : Tavily API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape descriptions (Tavily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Similarly, we look for info about addresses with Tavily API, rather than with a Google Search.\n",
    "\"\"\"\n",
    "\n",
    "from tavily import TavilyClient\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Initialize Tavily Client\n",
    "client = TavilyClient(\"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\")\n",
    "\n",
    "file_path = '../datasets/filtered_data_with_tavily_results.csv'\n",
    "if os.path.exists(file_path):\n",
    "    filtered_data = pd.read_csv(file_path)\n",
    "    if 'search_results' not in filtered_data.columns:\n",
    "        filtered_data['search_results'] = \"\"\n",
    "else:\n",
    "    filtered_data = pd.read_csv('../datasets/filtered_data_with_addresses.csv')  # Original dataset\n",
    "    filtered_data['search_results'] = \"\"\n",
    "\n",
    "# Function to search address directly and fetch content\n",
    "def fetch_address_info(address):\n",
    "    try:\n",
    "        response = client.search(query=address, max_results=5)\n",
    "        content = \" | \".join([result['content'] for result in response['results']])\n",
    "        return content if content else \"No content found\"\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing address '{address}': {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "# Iterate and save progress on each run\n",
    "for idx, row in tqdm(filtered_data.iterrows(), total=filtered_data.shape[0], desc=\"Fetching Tavily Results\"):\n",
    "    if pd.notna(row['search_results']) and row['search_results'].strip() != \"\":\n",
    "        continue  # Skip already processed\n",
    "\n",
    "    address = row['address']\n",
    "    result = fetch_address_info(address)\n",
    "    filtered_data.at[idx, 'search_results'] = result\n",
    "\n",
    "    # Save progress after each request\n",
    "    filtered_data.to_csv(file_path, index=False)\n",
    "\n",
    "    # Respect the 100 RPM rate limit\n",
    "    time.sleep(0.6)\n",
    "\n",
    "print(f\"✅ Tavily search results completed and saved to '{file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine whether the address is residential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We use an LLM to determine whether an address is residential, based on the description we retrieved from Google (or Tavily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = pd.read_csv('../datasets/filtered_data_with_google_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Initialize Gemini Flash model\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.0,\n",
    "    google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Define Pydantic model for structured response\n",
    "class Classification(BaseModel):\n",
    "    is_residential: int = Field(description=\"1 for residential, 0 for non-residential\")\n",
    "\n",
    "# LangChain parser\n",
    "parser = PydanticOutputParser(pydantic_object=Classification)\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are given multiple web search results about a single physical address:\n",
    "{search_results}\n",
    "\n",
    "Based on this information, determine whether the location is **residential** (e.g., a home or apartment) or **non-residential** (e.g., business, school, hospital, etc.). Do not consider special events as being non-residential.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "    input_variables=[\"search_results\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Create chain: prompt → model\n",
    "chain = prompt | model\n",
    "\n",
    "# Classification function\n",
    "def classify_address(search_results, address):\n",
    "    if pd.isna(search_results) or search_results.strip() == \"No results found\":\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    try:\n",
    "        output = chain.invoke({\"search_results\": search_results})\n",
    "        result = parser.invoke(output)\n",
    "        return result.is_residential\n",
    "    except ValidationError as ve:\n",
    "        print(f\"⚠️ Validation error for address '{address}': {ve}\")\n",
    "        return \"Error\"\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error classifying address '{address}': {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "# Multi-threaded classification\n",
    "def classify_addresses_concurrently(data, max_threads=10):\n",
    "    results = [\"Unknown\"] * len(data)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "        futures = {\n",
    "            executor.submit(classify_address, row['search_results'], row['address']): idx\n",
    "            for idx, row in data.iterrows()\n",
    "        }\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Classifying Addresses\"):\n",
    "            idx = futures[future]\n",
    "            try:\n",
    "                results[idx] = future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Thread error at index {idx}: {e}\")\n",
    "                results[idx] = \"Error\"\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run classification\n",
    "filtered_data[\"is_residential\"] = classify_addresses_concurrently(filtered_data)\n",
    "\n",
    "# Save output\n",
    "filtered_data.to_csv(\"../datasets/filtered_data_with_residential_classification.csv\", index=False)\n",
    "print(\"✅ Saved to 'filtered_data_with_residential_classification.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine whether it is the main address : All variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have indicators, and we know whether an address is residential, we can assess whether an address is the main address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "# ----------------------------\n",
    "# Load API key\n",
    "# ----------------------------\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ----------------------------\n",
    "# Init LLM\n",
    "# ----------------------------\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o\",\n",
    "    temperature=0.0,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Output schema\n",
    "# ----------------------------\n",
    "class MainClusterSelection(BaseModel):\n",
    "    main_cluster: int = Field(description=\"Cluster number selected as main address\")\n",
    "    justification: str = Field(description=\"Why this cluster was selected\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MainClusterSelection)\n",
    "\n",
    "# ----------------------------\n",
    "# Prompt Template\n",
    "# ----------------------------\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Each user (identified by a CAID) has multiple location clusters detected from mobile signals. Each cluster represents a place where the user spent time during Q2.\n",
    "\n",
    "Your task is to select the **main home address cluster** for the user based on the features below. We would like to differentiate from all other addresses, especially from work address.\n",
    "\n",
    "A main address is usually:\n",
    "- High **night** or **evening** consistency\n",
    "- High unique hours, meaning that the user is active at different times of the day\n",
    "- High consecutive hours, meaning that the user is active for a long time\n",
    "- Long stays and broad time window coverage\n",
    "- High dominance score \n",
    "- High total pings and diverse hourly activity (entropy)\n",
    "\n",
    "### Column Descriptions\n",
    "\n",
    "cluster: Cluster index for this user  \n",
    "is_residential: 1 for residential, 0 for non-residential  \n",
    "night_consistency_score: % of nights this cluster was seen (NaN if no night pings for the user)  \n",
    "evening_consistency_score: % of evenings this cluster was seen (NaN if no evening pings for the user)  \n",
    "day_consistency_score: % of days this cluster was seen (NaN if no day pings for the user)  \n",
    "dominance_score: % of device pings in this cluster  \n",
    "total_pings: Total number of pings in this cluster  \n",
    "unique_hours: Number of unique hourly bins this cluster was active  \n",
    "hour_entropy: Entropy of hourly activity (NaN if too few pings)  \n",
    "max_consecutive_hours: Longest continuous stay in hours  \n",
    "time_window_coverage: Fraction of [day, evening, night] time windows with activity\n",
    "\n",
    "---\n",
    "\n",
    "### Cluster Candidates\n",
    "\n",
    "{cluster_table}\n",
    "\n",
    "Choose the main_cluster and explain why.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "    input_variables=[\"cluster_table\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "# ----------------------------\n",
    "# Format one user's cluster group\n",
    "# ----------------------------\n",
    "def format_cluster_group(group):\n",
    "    rows = []\n",
    "    for _, row in group.iterrows():\n",
    "        rows.append(\n",
    "            f\"cluster: {row['cluster']}, is_residential: {row['is_residential']}, \"\n",
    "            f\"night_consistency_score: {row['night_consistency_score']}, \"\n",
    "            f\"evening_consistency_score: {row['evening_consistency_score']}, \"\n",
    "            f\"day_consistency_score: {row['day_consistency_score']}, \"\n",
    "            f\"dominance_score: {row['dominance_score']}, \"\n",
    "            f\"total_pings: {row['total_pings']}, \"\n",
    "            f\"unique_hours: {row['unique_hours']}, \"\n",
    "            f\"hour_entropy: {row['hour_entropy']}, \"\n",
    "            f\"max_consecutive_hours: {row['max_consecutive_hours']}, \"\n",
    "            f\"time_window_coverage: {row['time_window_coverage']}\"\n",
    "        )\n",
    "    return \"\\n\".join(rows)\n",
    "\n",
    "# ----------------------------\n",
    "# Decide main cluster for one user\n",
    "# ----------------------------\n",
    "def decide_main_cluster(caid, group):\n",
    "    try:\n",
    "        table = format_cluster_group(group)\n",
    "        output = chain.invoke({\"cluster_table\": table})\n",
    "        parsed = parser.invoke(output)\n",
    "        return {\n",
    "            \"caid\": caid,\n",
    "            \"quarter\": group.iloc[0][\"quarter\"],\n",
    "            \"cluster\": parsed.main_cluster,\n",
    "            \"is_main_address\": 1,\n",
    "            \"main_address_justification\": parsed.justification\n",
    "        }\n",
    "    except ValidationError as ve:\n",
    "        print(f\"❌ Validation error for caid {caid}: {ve}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for caid {caid}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ----------------------------\n",
    "# Load dataset\n",
    "# ----------------------------\n",
    "df = pd.read_csv(\"../datasets/filtered_data_with_residential_classification.csv\")\n",
    "\n",
    "# ----------------------------\n",
    "# Multi-threaded execution\n",
    "# ----------------------------\n",
    "results = []\n",
    "caid_groups = list(df.groupby(\"caid\"))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = {\n",
    "        executor.submit(decide_main_cluster, caid, group): caid\n",
    "        for caid, group in caid_groups\n",
    "    }\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Classifying main clusters\"):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "# ----------------------------\n",
    "# Merge and save\n",
    "# ----------------------------\n",
    "df_main = pd.DataFrame(results)\n",
    "\n",
    "df = df.merge(df_main, on=[\"caid\", \"quarter\", \"cluster\"], how=\"left\")\n",
    "df[\"is_main_address\"] = df[\"is_main_address\"].fillna(0).astype(int)\n",
    "df[\"main_address_justification\"] = df[\"main_address_justification\"].fillna(\"\")\n",
    "\n",
    "df.to_csv(\"../datasets/filtered_data_with_main_address_per_user.csv\", index=False)\n",
    "print(\"✅ Saved to 'filtered_data_with_main_address_per_user.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "df.to_excel(\"../datasets/filtered_data_with_main_address_per_user.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine whether it is the main address : All variables except is_residential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "# ----------------------------\n",
    "# Load API key\n",
    "# ----------------------------\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ----------------------------\n",
    "# Init LLM\n",
    "# ----------------------------\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o\",\n",
    "    temperature=0.0,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Output schema\n",
    "# ----------------------------\n",
    "class MainClusterSelection(BaseModel):\n",
    "    main_cluster: int = Field(description=\"Cluster number selected as main address\")\n",
    "    justification: str = Field(description=\"Why this cluster was selected\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MainClusterSelection)\n",
    "\n",
    "# ----------------------------\n",
    "# Prompt Template\n",
    "# ----------------------------\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Each user (identified by a CAID) has multiple location clusters detected from mobile signals. Each cluster represents a place where the user spent time during Q2.\n",
    "\n",
    "Your task is to select the **main home address cluster** for the user based on the features below. We would like to differentiate from all other addresses, especially from work address.\n",
    "\n",
    "A main address is usually:\n",
    "- High **night** or **evening** consistency\n",
    "- High unique hours, meaning that the user is active at different times of the day\n",
    "- High consecutive hours, meaning that the user is active for a long time\n",
    "- Long stays and broad time window coverage\n",
    "- High dominance score\n",
    "- High total pings and diverse hourly activity (entropy)\n",
    "\n",
    "### Column Descriptions\n",
    "\n",
    "cluster: Cluster index for this user  \n",
    "night_consistency_score: % of nights this cluster was seen (NaN if no night pings for the user)  \n",
    "evening_consistency_score: % of evenings this cluster was seen (NaN if no evening pings for the user)  \n",
    "day_consistency_score: % of days this cluster was seen (NaN if no day pings for the user)  \n",
    "dominance_score: % of device pings in this cluster  \n",
    "total_pings: Total number of pings in this cluster  \n",
    "unique_hours: Number of unique hourly bins this cluster was active  \n",
    "hour_entropy: Entropy of hourly activity (NaN if too few pings)  \n",
    "max_consecutive_hours: Longest continuous stay in hours  \n",
    "time_window_coverage: Fraction of [day, evening, night] time windows with activity\n",
    "\n",
    "---\n",
    "\n",
    "### Cluster Candidates\n",
    "\n",
    "{cluster_table}\n",
    "\n",
    "Choose the main_cluster and explain why.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "    input_variables=[\"cluster_table\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "# ----------------------------\n",
    "# Format a CAID group\n",
    "# ----------------------------\n",
    "def format_cluster_group(group):\n",
    "    return \"\\n\".join(\n",
    "        f\"cluster: {row['cluster']}, night_consistency_score: {row['night_consistency_score']}, \"\n",
    "        f\"evening_consistency_score: {row['evening_consistency_score']}, day_consistency_score: {row['day_consistency_score']}, \"\n",
    "        f\"dominance_score: {row['dominance_score']}, total_pings: {row['total_pings']}, \"\n",
    "        f\"unique_hours: {row['unique_hours']}, hour_entropy: {row['hour_entropy']}, \"\n",
    "        f\"max_consecutive_hours: {row['max_consecutive_hours']}, time_window_coverage: {row['time_window_coverage']}\"\n",
    "        for _, row in group.iterrows()\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Decide main cluster (thread target)\n",
    "# ----------------------------\n",
    "def process_caid(caid, group):\n",
    "    try:\n",
    "        cluster_table = format_cluster_group(group)\n",
    "        output = chain.invoke({\"cluster_table\": cluster_table})\n",
    "        parsed = parser.invoke(output)\n",
    "        return {\n",
    "            \"caid\": caid,\n",
    "            \"quarter\": group.iloc[0][\"quarter\"],\n",
    "            \"cluster\": parsed.main_cluster,\n",
    "            \"is_main_address_no_residential\": 1,\n",
    "            \"main_address_justification_no_residential\": parsed.justification\n",
    "        }\n",
    "    except ValidationError as ve:\n",
    "        print(f\"❌ Validation error for caid {caid}: {ve}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for caid {caid}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ----------------------------\n",
    "# Load dataset\n",
    "# ----------------------------\n",
    "df = pd.read_csv(\"../datasets/filtered_data_with_main_address_per_user.csv\")\n",
    "\n",
    "# ----------------------------\n",
    "# Threaded execution\n",
    "# ----------------------------\n",
    "results = []\n",
    "futures = []\n",
    "max_workers = 20 # Adjust based on your API limits\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    for caid, group in df.groupby(\"caid\"):\n",
    "        futures.append(executor.submit(process_caid, caid, group))\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Running threaded LLM\"):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Merge results and save\n",
    "# ----------------------------\n",
    "df_new = pd.DataFrame(results)\n",
    "df = df.merge(df_new, on=[\"caid\", \"quarter\", \"cluster\"], how=\"left\")\n",
    "df[\"is_main_address_no_residential\"] = df[\"is_main_address_no_residential\"].fillna(0).astype(int)\n",
    "df[\"main_address_justification_no_residential\"] = df[\"main_address_justification_no_residential\"].fillna(\"\")\n",
    "df.to_csv(\"../datasets/filtered_data_with_main_address_per_user.csv\", index=False)\n",
    "\n",
    "print(\"✅ Threaded LLM results appended to filtered_data_with_main_address_per_user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"../datasets/filtered_data_with_main_address_per_user.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of is_residential : what clusters got different assignment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved all clusters for CAIDs with disagreement to:\n",
      "../datasets/main_address_differences_all_clusters.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset with both decisions\n",
    "df = pd.read_csv(\"../datasets/filtered_data_with_main_address_per_user.csv\")\n",
    "\n",
    "# Step 1: Get main cluster selections\n",
    "main_with = df[df[\"is_main_address\"] == 1][[\"caid\", \"quarter\", \"cluster\"]].rename(columns={\"cluster\": \"main_with_res\"})\n",
    "main_without = df[df[\"is_main_address_no_residential\"] == 1][[\"caid\", \"quarter\", \"cluster\"]].rename(columns={\"cluster\": \"main_without_res\"})\n",
    "\n",
    "# Step 2: Merge and find CAIDs with differing main clusters\n",
    "comparison = pd.merge(main_with, main_without, on=[\"caid\", \"quarter\"], how=\"inner\")\n",
    "diff_caids = comparison[comparison[\"main_with_res\"] != comparison[\"main_without_res\"]][[\"caid\", \"quarter\"]]\n",
    "\n",
    "# Step 3: Get all clusters for those CAIDs\n",
    "df_diff = df.merge(diff_caids, on=[\"caid\", \"quarter\"], how=\"inner\")\n",
    "\n",
    "# Step 4: Export to Excel\n",
    "output_path = \"../datasets/main_address_differences_all_clusters.xlsx\"\n",
    "df_diff.to_excel(output_path, index=False)\n",
    "\n",
    "# Optional: Display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(\"✅ Saved all clusters for CAIDs with disagreement to:\")\n",
    "print(output_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Number of CAIDs with differing main clusters: 16\n"
     ]
    }
   ],
   "source": [
    "print(f\"🔢 Number of CAIDs with differing main clusters: {diff_caids['caid'].nunique()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
